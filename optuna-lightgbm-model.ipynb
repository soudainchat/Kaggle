{"cells":[{"metadata":{},"cell_type":"markdown","source":"#  Simple example of lightgbm model with optuna"},{"metadata":{"trusted":true},"cell_type":"code","source":"import optuna.integration.lightgbm as lgb\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import KFold, GridSearchCV, cross_validate, train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom lightgbm import LGBMRegressor\n\nimport optuna\nimport functools\nimport gc\nimport time\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#--------------------------------------read the data--------------------------------------\n\ntrain = pd.read_csv('../input/tabular-playground-series-feb-2021/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-feb-2021/test.csv')\n\nX_train = train.drop(['id', 'target'], axis=1)\ny_train = train.target\nX_test = test.drop(['id'], axis=1)\n\ncat_cols = [feature for feature in train.columns if 'cat' in feature]\n\n#--------------------------------------encode categorical features--------------------------------------\n\ndef label_encoder(df):\n    for feature in cat_cols:\n        le = LabelEncoder()\n        le.fit(df[feature])\n        df[feature] = le.transform(df[feature])\n    return df\n\nX_train = label_encoder(X_train)\nX_test = label_encoder(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#--------------------------------------making cross validation folds--------------------------------------\n\nsplit = KFold(n_splits=5, shuffle=True, random_state=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#--------------------------------------optuna parameters function--------------------------------------\n\ndef lb_opt(trial):\n    \n    max_depth =  trial.suggest_int('max_depth',1,30),\n    subsample =  trial.suggest_uniform('subsample',0.001,1),\n    colsample_bytree = trial.suggest_uniform('colsample_bytree',0.001,1),\n    learning_rate = trial.suggest_loguniform('learning_rate',0.001,0.1),\n    num_leaves = trial.suggest_int('num_leaves',2,70),\n    reg_lambda = trial.suggest_int('reg_lambda',1,100),\n    reg_alpha = trial.suggest_int('reg_alpha',1,100),\n    min_child_samples = trial.suggest_int('min_child_samples',1,100),\n    max_bin = trial.suggest_int('max_bin',1,1000),\n    cat_smooth = trial.suggest_int('cat_smooth',1,100),\n    cat_l2 = trial.suggest_uniform('cat_l2',0.001,1.0),\n    random_state = trial.suggest_int('random_state',1,50),\n   \n\n    \n    lightgbm_tuna = LGBMRegressor(\n        verbosity = 0,\n        num_leaves=num_leaves, \n        max_depth=max_depth, \n        learning_rate=learning_rate, \n        n_estimators=20000, \n        min_child_samples=min_child_samples, \n        subsample=subsample, \n        colsample_bytree=colsample_bytree, \n        reg_alpha=reg_alpha, \n        reg_lambda=reg_lambda, \n        random_state=random_state, \n        metric='rmse'\n    )\n    \n    lightgbm_tuna.fit(X_train,y_train)\n    lb_predict_test = lightgbm_tuna.predict(X_test)\n    \nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(lb_opt, n_trials=10)\nprint(study.best_trial)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#--------------------------------------params we got--------------------------------------\n\nlgbm_params = {'max_depth': 16, \n                'subsample': 0.7, \n                'colsample_bytree': 0.2, \n                'learning_rate': 0.007,\n                'reg_lambda': 10.98, \n                'reg_alpha': 17.3, \n                'min_child_samples': 31, \n                'num_leaves': 100, \n                'max_bin': 600, \n                'cat_smooth': 81, \n                'cat_l2': 0.03, \n                'metric': 'rmse', \n                'n_jobs': -1, \n                'n_estimators': 20000}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_list_base = []\npreds_list_final_iteration = []\npreds_list_all = []\n\n#--------------------------------------split the train data on train and validation sets--------------------------------------\n\nfor train_idx, val_idx in split.split(X_train):\n    \n            X_tr = X_train.iloc[train_idx]\n            X_val = X_train.iloc[val_idx]\n            y_tr = y_train.iloc[train_idx]\n            y_val = y_train.iloc[val_idx]\n            \n#--------------------------------------fit the model with optune params--------------------------------------\n            \n            Model = LGBMRegressor(**lgbm_params).fit(X_tr, y_tr, eval_set=[(X_val, y_val)],\n                          eval_metric=['rmse'],\n                          early_stopping_rounds=250, \n                          categorical_feature=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n                          #callbacks=[optuna.integration.LightGBMPruningCallback(trial, metric='rmse')],\n                          verbose=0)\n            \n            preds_list_base.append(Model.predict(X_test))\n            preds_list_all.append(Model.predict(X_test))\n            print(f'RMSE for Base model is {np.sqrt(mean_squared_error(y_val, Model.predict(X_val)))}')\n            first_rmse = np.sqrt(mean_squared_error(y_val, Model.predict(X_val)))\n            params = lgbm_params.copy()\n            \n#--------------------------------------reducing regularizing params if--------------------------------------            \n            for i in range(1, 8):\n                if i >2:    \n                    params['reg_lambda'] *= 0.9\n                    params['reg_alpha'] *= 0.9\n                    params['num_leaves'] += 40\n                    \n                params['learning_rate'] = 0.003\n                Model = LGBMRegressor(**params).fit(X_tr, y_tr, eval_set=[(X_val, y_val)],\n                          eval_metric=['rmse'],\n                          early_stopping_rounds=200, \n                          categorical_feature=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n                          verbose=0,\n                          init_model=Model)\n                \n                preds_list_all.append(Model.predict(X_test))\n                print(f'RMSE for Incremental trial {i} model is {np.sqrt(mean_squared_error(y_val, Model.predict(X_val)))}')\n                \n            last_rmse = np.sqrt(mean_squared_error(y_val, Model.predict(X_val)))\n            print('',end='\\n\\n')\n            print(f'Improvement of : {first_rmse - last_rmse}')\n            print('-' * 100)\n            preds_list_final_iteration.append(Model.predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_preds_final_iteration = np.array(preds_list_final_iteration).mean(axis=0)\ny_preds_final_iteration","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({'id':test.id,\n              'target':y_preds_final_iteration})\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}